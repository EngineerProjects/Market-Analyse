# Enterprise AI Configuration (YAML)

# LLM Configuration
llm:
  model: "llama3"  # Default model
  base_url: "http://localhost:11434"  # Default base URL
  api_key: ""  # Not needed for Ollama
  max_tokens: 4096
  temperature: 0.7
  api_type: "ollama"
  api_version: ""

  # Provider-specific overrides with all necessary parameters
  ollama:
    model: "llama3"
    base_url: "http://localhost:11434"  # Ollama-specific base URL
    api_key: ""  # Not needed for Ollama
    api_type: "ollama"
    temperature: 0.7
    # Ollama-specific parameters
    num_ctx: 8192           # Context window size
    num_predict: 4096       # Maximum tokens to generate
    num_gpu: 1              # Number of GPUs to use
    repeat_penalty: 1.1     # Penalty for repeating tokens
    num_thread: 8           # Number of threads
    mirostat: 0             # Sampling method
    top_k: 40               # Top-k sampling
    top_p: 0.9              # Top-p sampling
    seed: 42                # Random seed
    stop: ["\n\n", "```"]   # Stop sequences

  gpt4:
    model: "gpt-4-1106-preview"
    base_url: "https://api.openai.com/v1"
    api_key: ""  # Set via environment variable OPENAI_API_KEY
    temperature: 0.5
    api_type: "openai"

  claude:
    model: "claude-3-opus-20240229"
    base_url: "https://api.anthropic.com/v1"
    api_key: ""  # Set via environment variable ANTHROPIC_API_KEY
    api_type: "anthropic"

# LLM Service Configuration
llm_service:
  default_provider: "ollama"
  default_model: "llama3"
  connection_pool_size: 20
  validate_model: false
  enable_metrics: true
  enable_provider_pooling: true
  provider_pool_size: [2, 5]  # Min and max providers in pool

# Cache Configuration
cache:
  use_cache: true
  cache_type: "hybrid"
  ttl: 86400  # 24 hours
  max_size_mb: 500
  cache_dir: "./workspace/cache/llm"
  promotion_policy: "both"
  synchronize_writes: false
  retention: "7 days"

# Timeout Configuration
timeouts:
  default_timeout: 60.0  # seconds
  streaming_timeout: 300.0  # 5 minutes
  connect_timeout: 30.0
  read_timeout: 90.0
  write_timeout: 30.0
  pool_timeout: 60.0

# Model Selection Strategy
model_selection:
  preferred_model: "llama3"
  fallback_models: ["llama3.2", "llama2"]
  auto_fallback: true
  fallback_across_providers: true
  provider_preferences:
    - "ollama"
    - "openai"
    - "anthropic"
  capability_requirements:
    vision: false
    tools: true

# Request Orchestration
orchestrator:
  max_concurrent_requests: 20
  max_queue_size: 100
  adaptive_scaling: true
  max_retries: 3
  enable_deduplication: true
  enable_circuit_breaker: true
  circuit_breaker_threshold: 5
  circuit_breaker_reset_timeout: 300

# Ollama-specific Configuration (Global settings for Ollama provider)
ollama:
  auto_pull: true                # Automatically pull models if not available
  timeout: 900.0                 # 15 minutes timeout for model operations
  fallback_model: "llama3"       # Fallback model if requested model is unavailable
  model_cache_size: 3            # Maximum number of models to keep loaded
  connection_pool_size: 10       # Connection pool size for Ollama API
  keep_alive: true               # Keep models loaded in memory
  strict_validation: false       # Whether to strictly validate models (raises exceptions)
  host: "localhost"              # Ollama server host
  port: 11434                    # Ollama server port
  secure: false                  # Whether to use HTTPS for Ollama connections

# Sandbox Configuration
sandbox:
  use_sandbox: true
  image: "python:3.12-slim"
  work_dir: "/workspace"
  memory_limit: "512m"
  cpu_limit: 1.0
  timeout: 300
  network_enabled: false

# Browser Configuration
browser:
  headless: true
  disable_security: false
  extra_chromium_args: []
  max_content_length: 2000

  # Optional proxy configuration
  proxy:
    server: ""  # Proxy server address
    username: ""  # Optional username
    password: ""  # Optional password

# Search Configuration
search:
  engine: "Google"
  fallback_engines:
    - "DuckDuckGo"
    - "Bing"
  retry_delay: 60
  max_retries: 3

# Team Configuration
team:
  max_team_size: 10
  default_roles:
    - "manager"
    - "developer"
    - "researcher"
    - "analyst"
  role_templates_dir: "team_templates"

# Workspace path
workspace_root: "./workspace"
